
import streamlit as st
from typing import Dict, Any, Optional


def render_recognition_sidebar(
    container: Optional[Any] = None,
    default_min_confidence: float = 0.25,
    default_use_vlm: bool = False,
    default_use_rag: bool = False
) -> Dict[str, Any]:
    """
    Render recognition controls in the sidebar.

    Returns:
        Dict[str, Any]: Settings for recognition.
    """
    target = container if container is not None else st.sidebar

    if "use_vlm" not in st.session_state:
        st.session_state.use_vlm = default_use_vlm
    if "use_rag" not in st.session_state:
        st.session_state.use_rag = default_use_rag
    if "min_confidence" not in st.session_state:
        st.session_state.min_confidence = default_min_confidence

    with target:
        st.title("ğŸ”§ ç³»çµ±è¨­å®š")

        use_vlm = st.checkbox(
            "å•Ÿç”¨ VLM è¦–è¦ºèªè¨€æ¨¡å‹",
            value=st.session_state.use_vlm
        )
        st.session_state.use_vlm = use_vlm

        use_rag = st.checkbox(
            "â””â”€ é–‹å•ŸçŸ¥è­˜åº«è¼”åŠ© (RAG)",
            value=st.session_state.use_rag if use_vlm else False,
            disabled=not use_vlm
        )
        st.session_state.use_rag = use_rag if use_vlm else False

        min_confidence = st.slider(
            "ä¿¡å¿ƒåº¦é–€æª»",
            min_value=0.1,
            max_value=0.9,
            value=st.session_state.min_confidence,
            step=0.01,
            help="åªé¡¯ç¤ºè¶…éé–€æª»çš„è£½ç¨‹"
        )
        st.session_state.min_confidence = min_confidence

    return {
        "use_vlm": st.session_state.use_vlm,
        "use_rag": st.session_state.use_rag,
        "min_confidence": st.session_state.min_confidence
    }


def render_sidebar(engine):
    """
    æ¸²æŸ“å´é‚Šæ¬„ (System Info & LLM Settings)
    """
    with st.sidebar:
        st.header("System Info")

        # Access safely in case engine is still initializing or lib load failed
        try:
            schema_ver = engine.lib_manager.data.get('schema_version', 'N/A')
            libs = engine.lib_manager.data.get('libraries', {})
            official_count = len(libs.get('official', {}))
            contrib_count = len(libs.get('contributed', {}))
        except Exception:
            schema_ver, official_count, contrib_count = "Error", 0, 0

        st.caption(f"Ver: {schema_ver}")
        st.caption(f"Official: {official_count}")
        st.caption(f"Contrib: {contrib_count}")

        st.divider()

        st.header("LLM Settings")
        st.caption("Set up LLM API for intelligence")

        # Ensure session state exists
        if 'use_mock_llm' not in st.session_state:
            st.session_state.use_mock_llm = True
        if 'llm_api_key' not in st.session_state:
            st.session_state.llm_api_key = ""
        if 'llm_base_url' not in st.session_state:
            st.session_state.llm_base_url = "https://api.openai.com/v1"

        use_mock = st.toggle("ä½¿ç”¨ Mock æ¨¡å¼ (æ¸¬è©¦ç”¨)", value=st.session_state.use_mock_llm)
        st.session_state.use_mock_llm = use_mock

        if not use_mock:
            # 1. å®šç¾©é è¨­è¨­å®š (Presets) - Updated 2025.01 (Gemini 2.5 Era)
            PROVIDERS = {
                "Google Gemini": {
                    "url": "https://generativelanguage.googleapis.com/v1beta/openai/",
                    "models": [
                        "gemini-2.0-flash",
                        "gemini-2.0-flash-exp",
                        "gemini-2.5-flash",
                        "gemini-exp-1206"
                    ],
                    "help": "Gemini 1.5 å·²ç§»é™¤ã€‚è«‹ä½¿ç”¨ 2.0 æˆ– 2.5 ç³»åˆ—ã€‚"
                },
                "Groq": {
                    "url": "https://api.groq.com/openai/v1",
                    "models": ["llama-3.3-70b-versatile", "llama-3.1-8b-instant"],
                    "help": "é€Ÿåº¦æ¥µå¿« (LPU)ã€‚æ¨è–¦ Llama 3.3 æˆ– 3.1ã€‚"
                },
                "OpenAI (Official)": {
                    "url": "https://api.openai.com/v1",
                    "models": ["gpt-4o", "gpt-4o-mini"],
                    "help": "éœ€ç¶å®šä¿¡ç”¨å¡ã€‚æ¨è–¦ gpt-4o-mini (é«˜æ€§åƒ¹æ¯”)ã€‚"
                },
                "DeepSeek": {
                    "url": "https://api.deepseek.com",
                    "models": ["deepseek-chat", "deepseek-reasoner"],
                    "help": "chat (V3) é€šç”¨ï¼Œreasoner (R1) é©åˆè¤‡é›œæ¨ç†ã€‚"
                },
                "Local (LM Studio / Ollama)": {
                    "url": "http://localhost:1234/v1",
                    "models": ["local-model"],
                    "help": "æœ¬åœ°åŸ·è¡Œï¼Œéš±ç§æœ€å®‰å…¨ã€‚éœ€è‡ªè¡Œæ¶è¨­ Serverã€‚"
                },
                "Custom (è‡ªè¨‚)": {
                    "url": "",
                    "models": [],
                    "help": "æ‰‹å‹•è¼¸å…¥æ‰€æœ‰è¨­å®šã€‚"
                }
            }

            # 2. é¸æ“‡æœå‹™å•† (Provider)
            provider = st.selectbox(
                "é¸æ“‡æœå‹™å•† (Provider)",
                list(PROVIDERS.keys()),
                index=0,
                help="é¸æ“‡å¾Œæœƒè‡ªå‹•å¸¶å…¥å°æ‡‰çš„ URL èˆ‡æ¨¡å‹å»ºè­°"
            )

            selected_preset = PROVIDERS[provider]

            # 3. Base URL (Auto-filled but editable)
            # ä½¿ç”¨ key ä¾†å¼·åˆ¶æ›´æ–°é è¨­å€¼ï¼Œç•¶ provider æ”¹è®Šæ™‚
            base_url = st.text_input(
                "Base URL",
                value=selected_preset["url"],
                help=selected_preset["help"],
                key=f"url_{provider}"  # Trick to auto-update value when provider changes
            )

            # 4. API Key
            api_key = st.text_input("API Key", type="password", value=st.session_state.llm_api_key)

            # 5. Model Selection
            # åˆä½µé è¨­æ¨¡å‹èˆ‡ã€Œæ‰‹å‹•è¼¸å…¥ã€é¸é …
            model_options = selected_preset["models"] + ["Custom Input..."]

            selected_model_option = st.selectbox(
                "Model Name",
                model_options,
                help="Select a model or choose Custom Input"
            )

            if selected_model_option == "Custom Input...":
                final_model_name = st.text_input("Enter Model Name", placeholder="e.g. gpt-4-32k")
            else:
                final_model_name = selected_model_option

            # Connection Test Button
            if st.button("Test Connection", use_container_width=True):
                with st.spinner("Testing connection..."):
                    is_ok, msg = engine.prompt_master.test_connection(api_key, base_url, final_model_name)
                    if is_ok:
                        st.toast("Connection Successful")
                        st.success(f"Connection Verified\n\n- URL: OK\n- Key: OK\n- Model: {final_model_name}")
                    else:
                        st.toast("Connection Failed")
                        st.error(f"Connection Failed\n\n{msg}")

            # Save to Session State
            if api_key and final_model_name:
                st.session_state.llm_api_key = api_key
                st.session_state.llm_base_url = base_url
                st.session_state.llm_model_name = final_model_name

                # Show status
                st.caption(f"Provider: {provider}")
            else:
                st.warning("Please enter API Key and Model Name")

        st.divider()
        st.caption("NKUST Vision Lab")
